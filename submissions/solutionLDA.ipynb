{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_training = pd.read_csv('data/aps_failure_training_set.csv')\n",
    "set_training.pop('id')\n",
    "\n",
    "columns = pd.Series(set_training.columns)\n",
    "\n",
    "counts = columns.apply(lambda x: x.split(\"_\")[0]).value_counts().to_dict()\n",
    "classes_columns_idx = columns.apply(\n",
    "    lambda x: True if counts[x.split(\"_\")[0]] > 1 else False)\n",
    "\n",
    "classes_columns = columns[classes_columns_idx]\n",
    "classes_columns.apply(lambda x: x.split(\"_\")[0]).value_counts().to_dict()\n",
    "\n",
    "classes_trainning = set_training.copy()\n",
    "\n",
    "classes_trainning[\"class\"] = classes_trainning[\"class\"].replace(['neg', 'pos'], [0, 1])\n",
    "classes_trainning = classes_trainning.replace('na', 0).astype(float).corr()\n",
    "labels = set_training.pop('class')\n",
    "\n",
    "numerical_columns = columns[~classes_columns_idx]\n",
    "numerical_columns = numerical_columns[2:]\n",
    "\n",
    "classes_trainning[\"class\"].abs()[numerical_columns].sort_values(ascending=False).plot(kind='bar')\n",
    "column_relevant = classes_trainning[\"class\"].abs()[numerical_columns].sort_values(ascending=False) > 0.3\n",
    "\n",
    "relevant_numerical_columns = column_relevant[column_relevant].index\n",
    "\n",
    "\n",
    "other_columns = pd.Index(classes_columns).append(pd.Index(relevant_numerical_columns))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mt_er\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(multi_class=&#x27;multinomial&#x27;, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(multi_class=&#x27;multinomial&#x27;, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(multi_class='multinomial', random_state=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "target_value = labels.apply(lambda x: 1 if x == 'pos' else 0).values\n",
    "X = set_training[classes_columns].replace('na', 0).astype(int).values\n",
    "\n",
    "X_transformed = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, target_value, test_size=0.33, random_state=42, stratify=target_value)\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     19470\n",
      "           1       0.76      0.54      0.63       330\n",
      "\n",
      "    accuracy                           0.99     19800\n",
      "   macro avg       0.87      0.77      0.81     19800\n",
      "weighted avg       0.99      0.99      0.99     19800\n",
      "\n",
      "[[19413    57]\n",
      " [  152   178]]\n"
     ]
    }
   ],
   "source": [
    "target_prediction = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, target_prediction))\n",
    "print(confusion_matrix(y_test, target_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/aps_failure_test_set.csv')\n",
    "X = test_df[classes_columns].replace('na', 0).astype(int).values\n",
    "X_transformed = StandardScaler().fit_transform(X)\n",
    "target_prediction_ = clf.predict(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelize(x):\n",
    "    if x == 1:\n",
    "        return 'pos'\n",
    "    else:\n",
    "        return 'neg'\n",
    "labelize = np.vectorize(labelize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'id': np.arange(1,len(target_prediction_)+1),\"class\": labelize(target_prediction_)})\n",
    "output.to_csv('categorical values standardized logistic regression, output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functionLDA import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 97) (97, 70)\n"
     ]
    }
   ],
   "source": [
    "X = set_training[other_columns].replace('na', 0).astype(float).values\n",
    "X_transformed = StandardScaler().fit_transform(X)\n",
    "lda = LDA(70)\n",
    "lda.fit(X_transformed, target_value)\n",
    "X_transformed_ = lda.transform(X_transformed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed_, target_value, test_size=0.33, random_state=42, stratify=target_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mt_er\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:09:24] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     19470\n",
      "           1       0.81      0.65      0.72       330\n",
      "\n",
      "    accuracy                           0.99     19800\n",
      "   macro avg       0.90      0.82      0.86     19800\n",
      "weighted avg       0.99      0.99      0.99     19800\n",
      "\n",
      "[[19419    51]\n",
      " [  115   215]]\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "clf = XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='binary:logistic', booster='gbtree')\n",
    "clf.fit(X_train, y_train)\n",
    "target_prediction = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, target_prediction))\n",
    "\n",
    "print(confusion_matrix(y_test, target_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mt_er\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use keras to create a neural network\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "#use momentum optimizer\n",
    "from keras.optimizers.legacy import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight(classes=np.unique(target_value), y=target_value, class_weight='balanced')\n",
    "\n",
    "cw = {0: float(class_weights[0]), 1: float(class_weights[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = set_training[other_columns].replace('na', 0).astype(float).values\n",
    "X_transformed = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, target_value, test_size=0.33, random_state=42, stratify=target_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.7900 - val_loss: 0.6981\n",
      "Epoch 2/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7626 - val_loss: 0.6917\n",
      "Epoch 3/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7584 - val_loss: 0.6887\n",
      "Epoch 4/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7561 - val_loss: 0.6868\n",
      "Epoch 5/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7545 - val_loss: 0.6859\n",
      "Epoch 6/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7537 - val_loss: 0.6870\n",
      "Epoch 7/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7531 - val_loss: 0.6844\n",
      "Epoch 8/100\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.7526 - val_loss: 0.6839\n",
      "Epoch 9/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7532 - val_loss: 0.6840\n",
      "Epoch 10/100\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.7518 - val_loss: 0.6834\n",
      "Epoch 11/100\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.7517 - val_loss: 0.6836\n",
      "Epoch 12/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7513 - val_loss: 0.6830\n",
      "Epoch 13/100\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.7512 - val_loss: 0.6830\n",
      "Epoch 14/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7510 - val_loss: 0.6828\n",
      "Epoch 15/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7508 - val_loss: 0.6827\n",
      "Epoch 16/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7506 - val_loss: 0.6826\n",
      "Epoch 17/100\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.7504 - val_loss: 0.6822\n",
      "Epoch 18/100\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.7505 - val_loss: 0.6823\n",
      "Epoch 19/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7502 - val_loss: 0.6818\n",
      "Epoch 20/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7501 - val_loss: 0.6819\n",
      "Epoch 21/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7501 - val_loss: 0.6820\n",
      "Epoch 22/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7498 - val_loss: 0.6815\n",
      "Epoch 23/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7498 - val_loss: 0.6814\n",
      "Epoch 24/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7498 - val_loss: 0.6818\n",
      "Epoch 25/100\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.7497 - val_loss: 0.6815\n",
      "Epoch 26/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7495 - val_loss: 0.6815\n",
      "Epoch 27/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7494 - val_loss: 0.6812\n",
      "Epoch 28/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7495 - val_loss: 0.6818\n",
      "Epoch 29/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7493 - val_loss: 0.6811\n",
      "Epoch 30/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7494 - val_loss: 0.6812\n",
      "Epoch 31/100\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.7493 - val_loss: 0.6813\n",
      "Epoch 32/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7492 - val_loss: 0.6819\n",
      "Epoch 33/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7492 - val_loss: 0.6808\n",
      "Epoch 34/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7491 - val_loss: 0.6809\n",
      "Epoch 35/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7490 - val_loss: 0.6811\n",
      "Epoch 36/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7490 - val_loss: 0.6814\n",
      "Epoch 37/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7490 - val_loss: 0.6810\n",
      "Epoch 38/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7490 - val_loss: 0.6810\n",
      "Epoch 39/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7489 - val_loss: 0.6807\n",
      "Epoch 40/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7489 - val_loss: 0.6806\n",
      "Epoch 41/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7489 - val_loss: 0.6808\n",
      "Epoch 42/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7490 - val_loss: 0.6808\n",
      "Epoch 43/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7488 - val_loss: 0.6804\n",
      "Epoch 44/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7488 - val_loss: 0.6806\n",
      "Epoch 45/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7488 - val_loss: 0.6807\n",
      "Epoch 46/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7488 - val_loss: 0.6808\n",
      "Epoch 47/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7487 - val_loss: 0.6805\n",
      "Epoch 48/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7488 - val_loss: 0.6802\n",
      "Epoch 49/100\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.7486 - val_loss: 0.6804\n",
      "Epoch 50/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7487 - val_loss: 0.6803\n",
      "Epoch 51/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7488 - val_loss: 0.6824\n",
      "Epoch 52/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7488 - val_loss: 0.6803\n",
      "Epoch 53/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7487 - val_loss: 0.6811\n",
      "Epoch 54/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7487 - val_loss: 0.6803\n",
      "Epoch 55/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7486 - val_loss: 0.6802\n",
      "Epoch 56/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7485 - val_loss: 0.6803\n",
      "Epoch 57/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7485 - val_loss: 0.6799\n",
      "Epoch 58/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7485 - val_loss: 0.6804\n",
      "Epoch 59/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7485 - val_loss: 0.6802\n",
      "Epoch 60/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7486 - val_loss: 0.6801\n",
      "Epoch 61/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7486 - val_loss: 0.6801\n",
      "Epoch 62/100\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.7485 - val_loss: 0.6800\n",
      "Epoch 63/100\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.7484 - val_loss: 0.6798\n",
      "Epoch 64/100\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.7484 - val_loss: 0.6802\n",
      "Epoch 65/100\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.7484 - val_loss: 0.6800\n",
      "Epoch 66/100\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.7483 - val_loss: 0.6804\n",
      "Epoch 67/100\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.7484 - val_loss: 0.6802\n",
      "Epoch 68/100\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.7483 - val_loss: 0.6802\n",
      "Epoch 69/100\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.7484 - val_loss: 0.6799\n",
      "Epoch 70/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7489 - val_loss: 0.6805\n",
      "Epoch 71/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7484 - val_loss: 0.6801\n",
      "Epoch 72/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7485 - val_loss: 0.6803\n",
      "Epoch 73/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7483 - val_loss: 0.6800\n",
      "Epoch 74/100\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.7482 - val_loss: 0.6799\n",
      "Epoch 75/100\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.7482 - val_loss: 0.6799\n",
      "Epoch 76/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7482 - val_loss: 0.6798\n",
      "Epoch 77/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7482 - val_loss: 0.6807\n",
      "Epoch 78/100\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.7484 - val_loss: 0.6801\n",
      "Epoch 79/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7484 - val_loss: 0.6800\n",
      "Epoch 80/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7483 - val_loss: 0.6802\n",
      "Epoch 81/100\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.7483 - val_loss: 0.6796\n",
      "Epoch 82/100\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.7482 - val_loss: 0.6802\n",
      "Epoch 83/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7482 - val_loss: 0.6801\n",
      "Epoch 84/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7481 - val_loss: 0.6804\n",
      "Epoch 85/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7482 - val_loss: 0.6800\n",
      "Epoch 86/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7481 - val_loss: 0.6802\n",
      "Epoch 87/100\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.7482 - val_loss: 0.6804\n",
      "Epoch 88/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7482 - val_loss: 0.6808\n",
      "Epoch 89/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7481 - val_loss: 0.6801\n",
      "Epoch 90/100\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.7481 - val_loss: 0.6797\n",
      "Epoch 91/100\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.7484 - val_loss: 0.6802\n",
      "Epoch 92/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7480 - val_loss: 0.6797\n",
      "Epoch 93/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7480 - val_loss: 0.6799\n",
      "Epoch 94/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7481 - val_loss: 0.6800\n",
      "Epoch 95/100\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.7483 - val_loss: 0.6800\n",
      "Epoch 96/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7481 - val_loss: 0.6801\n",
      "Epoch 97/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7487 - val_loss: 0.6797\n",
      "Epoch 98/100\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.7480 - val_loss: 0.6799\n",
      "Epoch 99/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7479 - val_loss: 0.6798\n",
      "Epoch 100/100\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.7481 - val_loss: 0.6797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x14eb0cd09d0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create encoder decoder model for the data 6 layers deep\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_sigmoid(x):\n",
    "    return 2 * (1 / (1 + tf.exp(-x))) - 1\n",
    "\n",
    "input_df = Input(shape=(X_transformed.shape[1],))\n",
    "encoded = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-6),\n",
    "                activity_regularizer=regularizers.l1(1e-6))(input_df)\n",
    "\n",
    "encoded = Dense(48, activation='relu', kernel_regularizer=regularizers.l2(1e-6),\n",
    "                activity_regularizer=regularizers.l1(1e-6))(encoded)\n",
    "\n",
    "decoded = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-6),\n",
    "                activity_regularizer=regularizers.l1(1e-6))(encoded)\n",
    "decoded = Dense(X_transformed.shape[1], activation=custom_sigmoid)(decoded)\n",
    "\n",
    "autoencoder = Model(input_df, decoded)\n",
    "encoder = Model(input_df, encoded)\n",
    "adam = Adam(learning_rate=1e-3, clipvalue=0.5)\n",
    "autoencoder.compile(optimizer=adam, loss='mean_squared_error')\n",
    "autoencoder.fit(X_transformed, X_transformed, epochs=100, batch_size=128, validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1257/1257 [==============================] - 2s 1ms/step\n",
      "619/619 [==============================] - 1s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     19470\n",
      "           1       0.80      0.67      0.73       330\n",
      "\n",
      "    accuracy                           0.99     19800\n",
      "   macro avg       0.90      0.83      0.86     19800\n",
      "weighted avg       0.99      0.99      0.99     19800\n",
      "\n",
      "[[19414    56]\n",
      " [  109   221]]\n"
     ]
    }
   ],
   "source": [
    "#create XGBoost model\n",
    "import xgboost as xgb\n",
    "clf = xgb.XGBClassifier()\n",
    "clf.fit(encoder.predict(X_train), y_train)\n",
    "target_prediction = clf.predict(encoder.predict(X_test))\n",
    "\n",
    "print(classification_report(y_test, target_prediction))\n",
    "\n",
    "print(confusion_matrix(y_test, target_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a predictor deep neural network\n",
    "\n",
    "def get_predictor_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(32, input_dim=48,\n",
    "                    activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(1e-5),\n",
    "                    activity_regularizer=regularizers.l1(1e-5)))\n",
    "    \n",
    "    model.add(Dense(32, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(1e-5),\n",
    "                    activity_regularizer=regularizers.l1(1e-5)))\n",
    "    \n",
    "    model.add(Dense(32, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(1e-5),\n",
    "                    activity_regularizer=regularizers.l1(1e-5)))\n",
    "    model.add(Dense(32, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(1e-5),\n",
    "                    activity_regularizer=regularizers.l1(1e-5)))\n",
    "    \n",
    "    model.add(Dense(32, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(1e-5),\n",
    "                    activity_regularizer=regularizers.l1(1e-5)))\n",
    "    model.add(Dense(32, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(1e-5),\n",
    "                    activity_regularizer=regularizers.l1(1e-5)))\n",
    "    \n",
    "    model.add(Dense(32, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(1e-5),\n",
    "                    activity_regularizer=regularizers.l1(1e-5)))\n",
    "    model.add(Dense(32, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(1e-5),\n",
    "                    activity_regularizer=regularizers.l1(1e-5)))\n",
    "    model.add(Dense(32, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(1e-5),\n",
    "                    activity_regularizer=regularizers.l1(1e-5)))\n",
    "    model.add(Dense(32, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(1e-5),\n",
    "                    activity_regularizer=regularizers.l1(1e-5)))\n",
    "    model.add(Dense(32, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(1e-5),\n",
    "                    activity_regularizer=regularizers.l1(1e-5)))\n",
    "    model.add(Dense(16, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(1e-5),\n",
    "                    activity_regularizer=regularizers.l1(1e-5)))\n",
    "    \n",
    "    model.add(Dense(8, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(1e-5),\n",
    "                    activity_regularizer=regularizers.l1(1e-5)))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "\n",
    "    adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mt_er\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\legacy\\adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "preditctor = get_predictor_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1257/1257 [==============================] - 1s 1ms/step\n",
      "619/619 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "315/315 [==============================] - 4s 6ms/step - loss: 0.4150 - accuracy: 0.9828 - val_loss: 0.1183 - val_accuracy: 0.9833\n",
      "Epoch 2/100\n",
      "315/315 [==============================] - 2s 5ms/step - loss: 0.0603 - accuracy: 0.9846 - val_loss: 0.0439 - val_accuracy: 0.9849\n",
      "Epoch 3/100\n",
      "315/315 [==============================] - 2s 5ms/step - loss: 0.0420 - accuracy: 0.9853 - val_loss: 0.0403 - val_accuracy: 0.9859\n",
      "Epoch 4/100\n",
      "315/315 [==============================] - 2s 5ms/step - loss: 0.0387 - accuracy: 0.9863 - val_loss: 0.0389 - val_accuracy: 0.9862\n",
      "Epoch 5/100\n",
      "315/315 [==============================] - 1s 5ms/step - loss: 0.0369 - accuracy: 0.9874 - val_loss: 0.0378 - val_accuracy: 0.9872\n",
      "Epoch 6/100\n",
      "315/315 [==============================] - 2s 5ms/step - loss: 0.0352 - accuracy: 0.9879 - val_loss: 0.0365 - val_accuracy: 0.9876\n",
      "Epoch 7/100\n",
      "315/315 [==============================] - 1s 5ms/step - loss: 0.0340 - accuracy: 0.9886 - val_loss: 0.0362 - val_accuracy: 0.9876\n",
      "Epoch 8/100\n",
      "315/315 [==============================] - 2s 6ms/step - loss: 0.0330 - accuracy: 0.9895 - val_loss: 0.0354 - val_accuracy: 0.9879\n",
      "Epoch 9/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0323 - accuracy: 0.9896 - val_loss: 0.0344 - val_accuracy: 0.9883\n",
      "Epoch 10/100\n",
      "315/315 [==============================] - 1s 5ms/step - loss: 0.0315 - accuracy: 0.9898 - val_loss: 0.0337 - val_accuracy: 0.9888\n",
      "Epoch 11/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0309 - accuracy: 0.9901 - val_loss: 0.0339 - val_accuracy: 0.9893\n",
      "Epoch 12/100\n",
      "315/315 [==============================] - 1s 5ms/step - loss: 0.0303 - accuracy: 0.9903 - val_loss: 0.0332 - val_accuracy: 0.9890\n",
      "Epoch 13/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0298 - accuracy: 0.9906 - val_loss: 0.0334 - val_accuracy: 0.9888\n",
      "Epoch 14/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0292 - accuracy: 0.9908 - val_loss: 0.0336 - val_accuracy: 0.9886\n",
      "Epoch 15/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0288 - accuracy: 0.9911 - val_loss: 0.0327 - val_accuracy: 0.9894\n",
      "Epoch 16/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0282 - accuracy: 0.9911 - val_loss: 0.0328 - val_accuracy: 0.9889\n",
      "Epoch 17/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0280 - accuracy: 0.9912 - val_loss: 0.0329 - val_accuracy: 0.9888\n",
      "Epoch 18/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0278 - accuracy: 0.9912 - val_loss: 0.0335 - val_accuracy: 0.9891\n",
      "Epoch 19/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0273 - accuracy: 0.9913 - val_loss: 0.0332 - val_accuracy: 0.9892\n",
      "Epoch 20/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0268 - accuracy: 0.9914 - val_loss: 0.0340 - val_accuracy: 0.9895\n",
      "Epoch 21/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0266 - accuracy: 0.9917 - val_loss: 0.0324 - val_accuracy: 0.9893\n",
      "Epoch 22/100\n",
      "315/315 [==============================] - 2s 5ms/step - loss: 0.0260 - accuracy: 0.9914 - val_loss: 0.0327 - val_accuracy: 0.9896\n",
      "Epoch 23/100\n",
      "315/315 [==============================] - 1s 5ms/step - loss: 0.0257 - accuracy: 0.9916 - val_loss: 0.0346 - val_accuracy: 0.9897\n",
      "Epoch 24/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0257 - accuracy: 0.9918 - val_loss: 0.0340 - val_accuracy: 0.9898\n",
      "Epoch 25/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0252 - accuracy: 0.9921 - val_loss: 0.0333 - val_accuracy: 0.9898\n",
      "Epoch 26/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0249 - accuracy: 0.9921 - val_loss: 0.0341 - val_accuracy: 0.9901\n",
      "Epoch 27/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0245 - accuracy: 0.9923 - val_loss: 0.0338 - val_accuracy: 0.9890\n",
      "Epoch 28/100\n",
      "315/315 [==============================] - 2s 5ms/step - loss: 0.0242 - accuracy: 0.9925 - val_loss: 0.0337 - val_accuracy: 0.9898\n",
      "Epoch 29/100\n",
      "315/315 [==============================] - 1s 5ms/step - loss: 0.0242 - accuracy: 0.9923 - val_loss: 0.0333 - val_accuracy: 0.9894\n",
      "Epoch 30/100\n",
      "315/315 [==============================] - 2s 5ms/step - loss: 0.0234 - accuracy: 0.9926 - val_loss: 0.0339 - val_accuracy: 0.9898\n",
      "Epoch 31/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0232 - accuracy: 0.9928 - val_loss: 0.0333 - val_accuracy: 0.9897\n",
      "Epoch 32/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0233 - accuracy: 0.9926 - val_loss: 0.0342 - val_accuracy: 0.9901\n",
      "Epoch 33/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0228 - accuracy: 0.9930 - val_loss: 0.0334 - val_accuracy: 0.9898\n",
      "Epoch 34/100\n",
      "315/315 [==============================] - 2s 5ms/step - loss: 0.0230 - accuracy: 0.9930 - val_loss: 0.0341 - val_accuracy: 0.9899\n",
      "Epoch 35/100\n",
      "315/315 [==============================] - 2s 5ms/step - loss: 0.0225 - accuracy: 0.9931 - val_loss: 0.0339 - val_accuracy: 0.9896\n",
      "Epoch 36/100\n",
      "315/315 [==============================] - 2s 5ms/step - loss: 0.0225 - accuracy: 0.9931 - val_loss: 0.0339 - val_accuracy: 0.9901\n",
      "Epoch 37/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0222 - accuracy: 0.9931 - val_loss: 0.0338 - val_accuracy: 0.9896\n",
      "Epoch 38/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0221 - accuracy: 0.9933 - val_loss: 0.0338 - val_accuracy: 0.9897\n",
      "Epoch 39/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0218 - accuracy: 0.9931 - val_loss: 0.0352 - val_accuracy: 0.9901\n",
      "Epoch 40/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0214 - accuracy: 0.9935 - val_loss: 0.0346 - val_accuracy: 0.9899\n",
      "Epoch 41/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0210 - accuracy: 0.9934 - val_loss: 0.0356 - val_accuracy: 0.9886\n",
      "Epoch 42/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0210 - accuracy: 0.9940 - val_loss: 0.0379 - val_accuracy: 0.9897\n",
      "Epoch 43/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0212 - accuracy: 0.9935 - val_loss: 0.0358 - val_accuracy: 0.9894\n",
      "Epoch 44/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0209 - accuracy: 0.9937 - val_loss: 0.0352 - val_accuracy: 0.9895\n",
      "Epoch 45/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0204 - accuracy: 0.9942 - val_loss: 0.0353 - val_accuracy: 0.9896\n",
      "Epoch 46/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0200 - accuracy: 0.9938 - val_loss: 0.0355 - val_accuracy: 0.9895\n",
      "Epoch 47/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0200 - accuracy: 0.9941 - val_loss: 0.0374 - val_accuracy: 0.9896\n",
      "Epoch 48/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0199 - accuracy: 0.9943 - val_loss: 0.0385 - val_accuracy: 0.9898\n",
      "Epoch 49/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0197 - accuracy: 0.9945 - val_loss: 0.0383 - val_accuracy: 0.9873\n",
      "Epoch 50/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0201 - accuracy: 0.9940 - val_loss: 0.0359 - val_accuracy: 0.9893\n",
      "Epoch 51/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0194 - accuracy: 0.9945 - val_loss: 0.0358 - val_accuracy: 0.9895\n",
      "Epoch 52/100\n",
      "315/315 [==============================] - 2s 5ms/step - loss: 0.0189 - accuracy: 0.9945 - val_loss: 0.0371 - val_accuracy: 0.9894\n",
      "Epoch 53/100\n",
      "315/315 [==============================] - 1s 5ms/step - loss: 0.0187 - accuracy: 0.9947 - val_loss: 0.0360 - val_accuracy: 0.9894\n",
      "Epoch 54/100\n",
      "315/315 [==============================] - 2s 5ms/step - loss: 0.0187 - accuracy: 0.9946 - val_loss: 0.0399 - val_accuracy: 0.9898\n",
      "Epoch 55/100\n",
      "315/315 [==============================] - 2s 6ms/step - loss: 0.0187 - accuracy: 0.9948 - val_loss: 0.0382 - val_accuracy: 0.9894\n",
      "Epoch 56/100\n",
      "315/315 [==============================] - 2s 5ms/step - loss: 0.0188 - accuracy: 0.9948 - val_loss: 0.0387 - val_accuracy: 0.9891\n",
      "Epoch 57/100\n",
      "315/315 [==============================] - 2s 6ms/step - loss: 0.0182 - accuracy: 0.9951 - val_loss: 0.0393 - val_accuracy: 0.9894\n",
      "Epoch 58/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0181 - accuracy: 0.9951 - val_loss: 0.0385 - val_accuracy: 0.9894\n",
      "Epoch 59/100\n",
      "315/315 [==============================] - 2s 6ms/step - loss: 0.0178 - accuracy: 0.9952 - val_loss: 0.0382 - val_accuracy: 0.9890\n",
      "Epoch 60/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0187 - accuracy: 0.9947 - val_loss: 0.0400 - val_accuracy: 0.9896\n",
      "Epoch 61/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0171 - accuracy: 0.9957 - val_loss: 0.0397 - val_accuracy: 0.9883\n",
      "Epoch 62/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0174 - accuracy: 0.9954 - val_loss: 0.0398 - val_accuracy: 0.9888\n",
      "Epoch 63/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0174 - accuracy: 0.9955 - val_loss: 0.0440 - val_accuracy: 0.9885\n",
      "Epoch 64/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0171 - accuracy: 0.9957 - val_loss: 0.0473 - val_accuracy: 0.9895\n",
      "Epoch 65/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0168 - accuracy: 0.9958 - val_loss: 0.0455 - val_accuracy: 0.9894\n",
      "Epoch 66/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0169 - accuracy: 0.9956 - val_loss: 0.0425 - val_accuracy: 0.9890\n",
      "Epoch 67/100\n",
      "315/315 [==============================] - 1s 5ms/step - loss: 0.0166 - accuracy: 0.9960 - val_loss: 0.0409 - val_accuracy: 0.9888\n",
      "Epoch 68/100\n",
      "315/315 [==============================] - 2s 5ms/step - loss: 0.0167 - accuracy: 0.9956 - val_loss: 0.0405 - val_accuracy: 0.9882\n",
      "Epoch 69/100\n",
      "315/315 [==============================] - 1s 5ms/step - loss: 0.0160 - accuracy: 0.9962 - val_loss: 0.0471 - val_accuracy: 0.9899\n",
      "Epoch 70/100\n",
      "315/315 [==============================] - 2s 5ms/step - loss: 0.0167 - accuracy: 0.9958 - val_loss: 0.0440 - val_accuracy: 0.9891\n",
      "Epoch 71/100\n",
      "315/315 [==============================] - 2s 5ms/step - loss: 0.0160 - accuracy: 0.9960 - val_loss: 0.0458 - val_accuracy: 0.9895\n",
      "Epoch 72/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0160 - accuracy: 0.9963 - val_loss: 0.0453 - val_accuracy: 0.9894\n",
      "Epoch 73/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0156 - accuracy: 0.9962 - val_loss: 0.0502 - val_accuracy: 0.9894\n",
      "Epoch 74/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0155 - accuracy: 0.9963 - val_loss: 0.0502 - val_accuracy: 0.9898\n",
      "Epoch 75/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0155 - accuracy: 0.9965 - val_loss: 0.0482 - val_accuracy: 0.9890\n",
      "Epoch 76/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0150 - accuracy: 0.9965 - val_loss: 0.0473 - val_accuracy: 0.9894\n",
      "Epoch 77/100\n",
      "315/315 [==============================] - 1s 5ms/step - loss: 0.0157 - accuracy: 0.9962 - val_loss: 0.0467 - val_accuracy: 0.9888\n",
      "Epoch 78/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0154 - accuracy: 0.9962 - val_loss: 0.0475 - val_accuracy: 0.9893\n",
      "Epoch 79/100\n",
      "315/315 [==============================] - 1s 5ms/step - loss: 0.0156 - accuracy: 0.9962 - val_loss: 0.0472 - val_accuracy: 0.9891\n",
      "Epoch 80/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0152 - accuracy: 0.9966 - val_loss: 0.0478 - val_accuracy: 0.9892\n",
      "Epoch 81/100\n",
      "315/315 [==============================] - 1s 5ms/step - loss: 0.0148 - accuracy: 0.9966 - val_loss: 0.0477 - val_accuracy: 0.9891\n",
      "Epoch 82/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0147 - accuracy: 0.9966 - val_loss: 0.0483 - val_accuracy: 0.9894\n",
      "Epoch 83/100\n",
      "315/315 [==============================] - 1s 5ms/step - loss: 0.0141 - accuracy: 0.9971 - val_loss: 0.0484 - val_accuracy: 0.9890\n",
      "Epoch 84/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0142 - accuracy: 0.9969 - val_loss: 0.0470 - val_accuracy: 0.9888\n",
      "Epoch 85/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0141 - accuracy: 0.9970 - val_loss: 0.0511 - val_accuracy: 0.9896\n",
      "Epoch 86/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0165 - accuracy: 0.9961 - val_loss: 0.0550 - val_accuracy: 0.9897\n",
      "Epoch 87/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0147 - accuracy: 0.9969 - val_loss: 0.0513 - val_accuracy: 0.9895\n",
      "Epoch 88/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0139 - accuracy: 0.9969 - val_loss: 0.0493 - val_accuracy: 0.9897\n",
      "Epoch 89/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0141 - accuracy: 0.9968 - val_loss: 0.0486 - val_accuracy: 0.9892\n",
      "Epoch 90/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0142 - accuracy: 0.9968 - val_loss: 0.0477 - val_accuracy: 0.9880\n",
      "Epoch 91/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0136 - accuracy: 0.9972 - val_loss: 0.0498 - val_accuracy: 0.9891\n",
      "Epoch 92/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0138 - accuracy: 0.9970 - val_loss: 0.0487 - val_accuracy: 0.9894\n",
      "Epoch 93/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0135 - accuracy: 0.9973 - val_loss: 0.0538 - val_accuracy: 0.9896\n",
      "Epoch 94/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0134 - accuracy: 0.9970 - val_loss: 0.0512 - val_accuracy: 0.9891\n",
      "Epoch 95/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0130 - accuracy: 0.9974 - val_loss: 0.0495 - val_accuracy: 0.9881\n",
      "Epoch 96/100\n",
      "315/315 [==============================] - 1s 5ms/step - loss: 0.0131 - accuracy: 0.9974 - val_loss: 0.0520 - val_accuracy: 0.9893\n",
      "Epoch 97/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0144 - accuracy: 0.9968 - val_loss: 0.0545 - val_accuracy: 0.9900\n",
      "Epoch 98/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0133 - accuracy: 0.9971 - val_loss: 0.0517 - val_accuracy: 0.9893\n",
      "Epoch 99/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0129 - accuracy: 0.9972 - val_loss: 0.0547 - val_accuracy: 0.9895\n",
      "Epoch 100/100\n",
      "315/315 [==============================] - 1s 4ms/step - loss: 0.0131 - accuracy: 0.9973 - val_loss: 0.0546 - val_accuracy: 0.9901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x14eae087190>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preditctor.fit(encoder.predict(X_train), y_train, epochs=100, batch_size=128, validation_data=(encoder.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "619/619 [==============================] - 1s 1ms/step\n",
      "619/619 [==============================] - 1s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     19470\n",
      "           1       0.74      0.63      0.68       330\n",
      "\n",
      "    accuracy                           0.99     19800\n",
      "   macro avg       0.87      0.81      0.84     19800\n",
      "weighted avg       0.99      0.99      0.99     19800\n",
      "\n",
      "[[19396    74]\n",
      " [  123   207]]\n"
     ]
    }
   ],
   "source": [
    "target_prediction = preditctor.predict(encoder.predict(X_test))\n",
    "target_prediction = np.round(target_prediction)\n",
    "\n",
    "print(classification_report(y_test, target_prediction))\n",
    "\n",
    "print(confusion_matrix(y_test, target_prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 97)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add ln and exp of numerical columns and look at the correlation\n",
    "log_df = set_training[numerical_columns].replace('na', 0).astype(float).apply(np.log)\n",
    "log_df = log_df.replace(-np.inf, 0)\n",
    "log_df = log_df.replace(np.inf, 0)\n",
    "log_df = log_df.replace(np.nan, 0)\n",
    "\n",
    "exp_df = set_training[numerical_columns].replace('na', 0).astype(float).apply(np.exp)\n",
    "exp_df = exp_df.replace(-np.inf, 0)\n",
    "exp_df = exp_df.replace(np.inf, 0)\n",
    "exp_df = exp_df.replace(np.nan, 0)\n",
    "\n",
    "#create a new pandas df with the log and exp columns and name them accordingly\n",
    "log_df.columns = log_df.columns.map(lambda x: 'log_' + x)\n",
    "exp_df.columns = exp_df.columns.map(lambda x: 'exp_' + x)\n",
    "\n",
    "#concatenate the log and exp columns with the original df\n",
    "set_training = pd.concat([set_training, log_df, exp_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr matrix\n",
    "\n",
    "classes_trainning = set_training.copy()\n",
    "\n",
    "#look at the correlation between the columns against the targe\n",
    "classes_trainning[\"label\"] = labels.apply(lambda x: 1 if x == 'pos' else 0)\n",
    "classes_trainning = classes_trainning.replace('na', 0).astype(float).corr()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "classes_trainning[\"label\"].abs()[numerical_columns].sort_values(ascending=False).plot(kind='bar')\n",
    "column_relevant = classes_trainning[\"label\"].abs()[numerical_columns].sort_values(ascending=False) > 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the columns that are relevant >.2\n",
    "column_relevant = classes_trainning[\"label\"].abs()[numerical_columns].sort_values(ascending=False) > 0.20\n",
    "relevant_numerical_columns = column_relevant[column_relevant].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add two column types together\n",
    "other_columns = pd.Index(classes_columns).append(pd.Index(relevant_numerical_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create neural network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "def get_model_for_aug_data(input_size= len(other_columns)):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=input_size, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-5), activity_regularizer=regularizers.l1(1e-5)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-5), activity_regularizer=regularizers.l1(1e-5)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-5), activity_regularizer=regularizers.l1(1e-5)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-5), activity_regularizer=regularizers.l1(1e-5)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-5), activity_regularizer=regularizers.l1(1e-5)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-5), activity_regularizer=regularizers.l1(1e-5)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-5), activity_regularizer=regularizers.l1(1e-5)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-5), activity_regularizer=regularizers.l1(1e-5)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-5), activity_regularizer=regularizers.l1(1e-5)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-5), activity_regularizer=regularizers.l1(1e-5)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-5), activity_regularizer=regularizers.l1(1e-5)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mt_er\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\legacy\\adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "aug_model = get_model_for_aug_data()\n",
    "X = set_training[other_columns].replace('na', 0).astype(float).values\n",
    "X_transformed = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, target_value, test_size=0.33, random_state=42, stratify=target_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "315/315 [==============================] - 5s 9ms/step - loss: 0.3568 - accuracy: 0.9698 - val_loss: 0.1047 - val_accuracy: 0.9833\n",
      "Epoch 2/100\n",
      "315/315 [==============================] - 3s 8ms/step - loss: 0.1330 - accuracy: 0.9833 - val_loss: 0.0715 - val_accuracy: 0.9833\n",
      "Epoch 3/100\n",
      "315/315 [==============================] - 3s 8ms/step - loss: 0.0962 - accuracy: 0.9833 - val_loss: 0.0609 - val_accuracy: 0.9833\n",
      "Epoch 4/100\n",
      "315/315 [==============================] - 3s 8ms/step - loss: 0.0838 - accuracy: 0.9833 - val_loss: 0.0577 - val_accuracy: 0.9833\n",
      "Epoch 5/100\n",
      "315/315 [==============================] - 3s 8ms/step - loss: 0.0765 - accuracy: 0.9833 - val_loss: 0.0560 - val_accuracy: 0.9833\n",
      "Epoch 6/100\n",
      "315/315 [==============================] - 3s 8ms/step - loss: 0.0720 - accuracy: 0.9833 - val_loss: 0.0542 - val_accuracy: 0.9833\n",
      "Epoch 7/100\n",
      "315/315 [==============================] - 2s 7ms/step - loss: 0.0663 - accuracy: 0.9833 - val_loss: 0.0531 - val_accuracy: 0.9833\n",
      "Epoch 8/100\n",
      "315/315 [==============================] - 2s 7ms/step - loss: 0.0642 - accuracy: 0.9833 - val_loss: 0.0521 - val_accuracy: 0.9833\n",
      "Epoch 9/100\n",
      "315/315 [==============================] - 2s 7ms/step - loss: 0.0629 - accuracy: 0.9833 - val_loss: 0.0511 - val_accuracy: 0.9833\n",
      "Epoch 10/100\n",
      "315/315 [==============================] - 2s 7ms/step - loss: 0.0595 - accuracy: 0.9833 - val_loss: 0.0501 - val_accuracy: 0.9833\n",
      "Epoch 11/100\n",
      "315/315 [==============================] - 2s 7ms/step - loss: 0.0597 - accuracy: 0.9833 - val_loss: 0.0491 - val_accuracy: 0.9833\n",
      "Epoch 12/100\n",
      "315/315 [==============================] - 2s 7ms/step - loss: 0.0575 - accuracy: 0.9833 - val_loss: 0.0485 - val_accuracy: 0.9833\n",
      "Epoch 13/100\n",
      "315/315 [==============================] - 2s 7ms/step - loss: 0.0550 - accuracy: 0.9833 - val_loss: 0.0478 - val_accuracy: 0.9833\n",
      "Epoch 14/100\n",
      "315/315 [==============================] - 3s 8ms/step - loss: 0.0551 - accuracy: 0.9833 - val_loss: 0.0470 - val_accuracy: 0.9833\n",
      "Epoch 15/100\n",
      "315/315 [==============================] - 3s 8ms/step - loss: 0.0545 - accuracy: 0.9833 - val_loss: 0.0466 - val_accuracy: 0.9833\n",
      "Epoch 16/100\n",
      "315/315 [==============================] - 3s 9ms/step - loss: 0.0541 - accuracy: 0.9833 - val_loss: 0.0458 - val_accuracy: 0.9833\n",
      "Epoch 17/100\n",
      "315/315 [==============================] - 2s 7ms/step - loss: 0.0513 - accuracy: 0.9833 - val_loss: 0.0453 - val_accuracy: 0.9833\n",
      "Epoch 18/100\n",
      "315/315 [==============================] - 2s 8ms/step - loss: 0.0516 - accuracy: 0.9833 - val_loss: 0.0446 - val_accuracy: 0.9833\n",
      "Epoch 19/100\n",
      "315/315 [==============================] - 2s 7ms/step - loss: 0.0500 - accuracy: 0.9833 - val_loss: 0.0438 - val_accuracy: 0.9833\n",
      "Epoch 20/100\n",
      "315/315 [==============================] - 2s 7ms/step - loss: 0.0501 - accuracy: 0.9834 - val_loss: 0.0432 - val_accuracy: 0.9833\n",
      "Epoch 21/100\n",
      "315/315 [==============================] - 2s 7ms/step - loss: 0.0487 - accuracy: 0.9835 - val_loss: 0.0426 - val_accuracy: 0.9833\n",
      "Epoch 22/100\n",
      "315/315 [==============================] - 2s 8ms/step - loss: 0.0469 - accuracy: 0.9839 - val_loss: 0.0422 - val_accuracy: 0.9843\n",
      "Epoch 23/100\n",
      "315/315 [==============================] - 2s 7ms/step - loss: 0.0461 - accuracy: 0.9844 - val_loss: 0.0415 - val_accuracy: 0.9860\n",
      "Epoch 24/100\n",
      "315/315 [==============================] - 2s 8ms/step - loss: 0.0468 - accuracy: 0.9845 - val_loss: 0.0404 - val_accuracy: 0.9876\n",
      "Epoch 25/100\n",
      "315/315 [==============================] - 2s 8ms/step - loss: 0.0466 - accuracy: 0.9851 - val_loss: 0.0398 - val_accuracy: 0.9878\n",
      "Epoch 26/100\n",
      "315/315 [==============================] - 2s 8ms/step - loss: 0.0447 - accuracy: 0.9854 - val_loss: 0.0396 - val_accuracy: 0.9878\n",
      "Epoch 27/100\n",
      "315/315 [==============================] - 2s 8ms/step - loss: 0.0444 - accuracy: 0.9853 - val_loss: 0.0391 - val_accuracy: 0.9879\n",
      "Epoch 28/100\n",
      "315/315 [==============================] - 3s 8ms/step - loss: 0.0437 - accuracy: 0.9859 - val_loss: 0.0386 - val_accuracy: 0.9882\n",
      "Epoch 29/100\n",
      "315/315 [==============================] - 2s 8ms/step - loss: 0.0441 - accuracy: 0.9863 - val_loss: 0.0386 - val_accuracy: 0.9881\n",
      "Epoch 30/100\n",
      "315/315 [==============================] - 2s 7ms/step - loss: 0.0417 - accuracy: 0.9863 - val_loss: 0.0382 - val_accuracy: 0.9883\n",
      "Epoch 31/100\n",
      "315/315 [==============================] - 2s 8ms/step - loss: 0.0428 - accuracy: 0.9866 - val_loss: 0.0383 - val_accuracy: 0.9884\n",
      "Epoch 32/100\n",
      "315/315 [==============================] - 2s 7ms/step - loss: 0.0429 - accuracy: 0.9873 - val_loss: 0.0379 - val_accuracy: 0.9885\n",
      "Epoch 33/100\n",
      "315/315 [==============================] - 2s 8ms/step - loss: 0.0420 - accuracy: 0.9869 - val_loss: 0.0373 - val_accuracy: 0.9891\n",
      "Epoch 34/100\n",
      "315/315 [==============================] - 2s 7ms/step - loss: 0.0414 - accuracy: 0.9874 - val_loss: 0.0371 - val_accuracy: 0.9892\n",
      "Epoch 35/100\n",
      "315/315 [==============================] - 2s 7ms/step - loss: 0.0414 - accuracy: 0.9869 - val_loss: 0.0370 - val_accuracy: 0.9894\n",
      "Epoch 36/100\n",
      "315/315 [==============================] - 2s 7ms/step - loss: 0.0414 - accuracy: 0.9874 - val_loss: 0.0373 - val_accuracy: 0.9891\n",
      "Epoch 37/100\n",
      "315/315 [==============================] - 2s 8ms/step - loss: 0.0393 - accuracy: 0.9873 - val_loss: 0.0365 - val_accuracy: 0.9897\n",
      "Epoch 38/100\n",
      "315/315 [==============================] - 2s 7ms/step - loss: 0.0404 - accuracy: 0.9882 - val_loss: 0.0365 - val_accuracy: 0.9897\n",
      "Epoch 39/100\n",
      "315/315 [==============================] - 2s 7ms/step - loss: 0.0398 - accuracy: 0.9878 - val_loss: 0.0364 - val_accuracy: 0.9897\n",
      "Epoch 40/100\n",
      "315/315 [==============================] - 2s 7ms/step - loss: 0.0390 - accuracy: 0.9888 - val_loss: 0.0359 - val_accuracy: 0.9897\n",
      "Epoch 41/100\n",
      "315/315 [==============================] - 2s 8ms/step - loss: 0.0382 - accuracy: 0.9887 - val_loss: 0.0361 - val_accuracy: 0.9895\n",
      "Epoch 42/100\n",
      "315/315 [==============================] - 2s 7ms/step - loss: 0.0385 - accuracy: 0.9888 - val_loss: 0.0353 - val_accuracy: 0.9904\n",
      "Epoch 43/100\n",
      "315/315 [==============================] - 2s 8ms/step - loss: 0.0385 - accuracy: 0.9892 - val_loss: 0.0355 - val_accuracy: 0.9899\n",
      "Epoch 44/100\n",
      "315/315 [==============================] - 2s 8ms/step - loss: 0.0378 - accuracy: 0.9886 - val_loss: 0.0356 - val_accuracy: 0.9899\n",
      "Epoch 45/100\n",
      "315/315 [==============================] - 2s 7ms/step - loss: 0.0373 - accuracy: 0.9889 - val_loss: 0.0350 - val_accuracy: 0.9901\n",
      "Epoch 46/100\n",
      "313/315 [============================>.] - ETA: 0s - loss: 0.0373 - accuracy: 0.9897"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43maug_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mt_er\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\mt_er\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:1856\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1841\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[0;32m   1842\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m   1843\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1854\u001b[0m         pss_evaluation_shards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pss_evaluation_shards,\n\u001b[0;32m   1855\u001b[0m     )\n\u001b[1;32m-> 1856\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1858\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1859\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1867\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1868\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1869\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1870\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1871\u001b[0m }\n\u001b[0;32m   1872\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32mc:\\Users\\mt_er\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\mt_er\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:2296\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   2292\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   2293\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2294\u001b[0m             ):\n\u001b[0;32m   2295\u001b[0m                 callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 2296\u001b[0m                 logs \u001b[38;5;241m=\u001b[39m \u001b[43mtest_function_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2297\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2298\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdata_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2299\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2300\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pss_evaluation_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2301\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2303\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2304\u001b[0m \u001b[38;5;66;03m# Override with model metrics instead of last step logs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mt_er\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:4108\u001b[0m, in \u001b[0;36m_TestFunction.run_step\u001b[1;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[0;32m   4107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_or_iterator, data_handler, step, unused_shards):\n\u001b[1;32m-> 4108\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   4110\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\mt_er\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\mt_er\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\mt_er\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:877\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 877\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    881\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    882\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mt_er\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mt_er\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\mt_er\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m   \u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\mt_er\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mt_er\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\mt_er\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "aug_model.fit(X_train, y_train, epochs=100, batch_size=128, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy model for fine tuning\n",
    "from keras.models import clone_model\n",
    "aug_model_2 = clone_model(aug_model)\n",
    "aug_model_2.set_weights(aug_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "619/619 [==============================] - 1s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     19470\n",
      "           1       0.83      0.48      0.61       330\n",
      "\n",
      "    accuracy                           0.99     19800\n",
      "   macro avg       0.91      0.74      0.80     19800\n",
      "weighted avg       0.99      0.99      0.99     19800\n",
      "\n",
      "[[19437    33]\n",
      " [  170   160]]\n"
     ]
    }
   ],
   "source": [
    "target_prediction = aug_model.predict(X_test)\n",
    "target_prediction = np.round(target_prediction)\n",
    "\n",
    "print(classification_report(y_test, target_prediction))\n",
    "\n",
    "print(confusion_matrix(y_test, target_prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mt_er\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mt_er\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\legacy\\adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#finetune with a diffent objective function\n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, target_prediction):\n",
    "    # Cast y_true to float32\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "\n",
    "    # Calculate the binary cross entropy\n",
    "    bce = K.binary_crossentropy(y_true, target_prediction)\n",
    "\n",
    "    # Define the weights\n",
    "    weight_for_false_negatives = K.constant(500.0)  # Use keras constant for float\n",
    "    weight_for_false_positives = K.constant(10.0)   # Use keras constant for float\n",
    "\n",
    "    # Apply the weights\n",
    "    weight_vector = y_true * weight_for_false_negatives + (1 - y_true) * weight_for_false_positives\n",
    "    weighted_bce = weight_vector * bce\n",
    "\n",
    "    return K.mean(weighted_bce)\n",
    "\n",
    "adam = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "\n",
    "aug_model.compile(optimizer='adam', loss=weighted_binary_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "619/619 [==============================] - 1s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     19470\n",
      "           1       0.83      0.48      0.61       330\n",
      "\n",
      "    accuracy                           0.99     19800\n",
      "   macro avg       0.91      0.74      0.80     19800\n",
      "weighted avg       0.99      0.99      0.99     19800\n",
      "\n",
      "[[19437    33]\n",
      " [  170   160]]\n"
     ]
    }
   ],
   "source": [
    "target_prediction = aug_model.predict(X_test)\n",
    "\n",
    "target_prediction = np.round(target_prediction)\n",
    "\n",
    "print(classification_report(y_test, target_prediction))\n",
    "\n",
    "print(confusion_matrix(y_test, target_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>aa_000</th>\n",
       "      <th>ab_000</th>\n",
       "      <th>ac_000</th>\n",
       "      <th>ad_000</th>\n",
       "      <th>ae_000</th>\n",
       "      <th>af_000</th>\n",
       "      <th>ag_000</th>\n",
       "      <th>ag_001</th>\n",
       "      <th>ag_002</th>\n",
       "      <th>...</th>\n",
       "      <th>ee_002</th>\n",
       "      <th>ee_003</th>\n",
       "      <th>ee_004</th>\n",
       "      <th>ee_005</th>\n",
       "      <th>ee_006</th>\n",
       "      <th>ee_007</th>\n",
       "      <th>ee_008</th>\n",
       "      <th>ee_009</th>\n",
       "      <th>ef_000</th>\n",
       "      <th>eg_000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1098</td>\n",
       "      <td>138</td>\n",
       "      <td>412</td>\n",
       "      <td>654</td>\n",
       "      <td>78</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1068</td>\n",
       "      <td>276</td>\n",
       "      <td>1620</td>\n",
       "      <td>116</td>\n",
       "      <td>86</td>\n",
       "      <td>462</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>66002</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>495076</td>\n",
       "      <td>380368</td>\n",
       "      <td>440134</td>\n",
       "      <td>269556</td>\n",
       "      <td>1315022</td>\n",
       "      <td>153680</td>\n",
       "      <td>516</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>59816</td>\n",
       "      <td>na</td>\n",
       "      <td>1010</td>\n",
       "      <td>936</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>540820</td>\n",
       "      <td>243270</td>\n",
       "      <td>483302</td>\n",
       "      <td>485332</td>\n",
       "      <td>431376</td>\n",
       "      <td>210074</td>\n",
       "      <td>281662</td>\n",
       "      <td>3232</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1814</td>\n",
       "      <td>na</td>\n",
       "      <td>156</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7646</td>\n",
       "      <td>4144</td>\n",
       "      <td>18466</td>\n",
       "      <td>49782</td>\n",
       "      <td>3176</td>\n",
       "      <td>482</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>15996</td>\n",
       "      <td>81852</td>\n",
       "      <td>na</td>\n",
       "      <td>2130706432</td>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>632658</td>\n",
       "      <td>273242</td>\n",
       "      <td>510354</td>\n",
       "      <td>373918</td>\n",
       "      <td>349840</td>\n",
       "      <td>317840</td>\n",
       "      <td>960024</td>\n",
       "      <td>25566</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>15997</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>46</td>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>266</td>\n",
       "      <td>44</td>\n",
       "      <td>46</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>15998</td>\n",
       "      <td>79636</td>\n",
       "      <td>na</td>\n",
       "      <td>1670</td>\n",
       "      <td>1518</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>806832</td>\n",
       "      <td>449962</td>\n",
       "      <td>778826</td>\n",
       "      <td>581558</td>\n",
       "      <td>375498</td>\n",
       "      <td>222866</td>\n",
       "      <td>358934</td>\n",
       "      <td>19548</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>15999</td>\n",
       "      <td>110</td>\n",
       "      <td>na</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>588</td>\n",
       "      <td>210</td>\n",
       "      <td>180</td>\n",
       "      <td>544</td>\n",
       "      <td>1004</td>\n",
       "      <td>1338</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15999</th>\n",
       "      <td>16000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>46</td>\n",
       "      <td>10</td>\n",
       "      <td>48</td>\n",
       "      <td>14</td>\n",
       "      <td>42</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16000 rows × 171 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  aa_000 ab_000      ac_000 ad_000 ae_000 af_000 ag_000 ag_001  \\\n",
       "0          1      60      0          20     12      0      0      0      0   \n",
       "1          2      82      0          68     40      0      0      0      0   \n",
       "2          3   66002      2         212    112      0      0      0      0   \n",
       "3          4   59816     na        1010    936      0      0      0      0   \n",
       "4          5    1814     na         156    140      0      0      0      0   \n",
       "...      ...     ...    ...         ...    ...    ...    ...    ...    ...   \n",
       "15995  15996   81852     na  2130706432    892      0      0      0      0   \n",
       "15996  15997      18      0          52     46      8     26      0      0   \n",
       "15997  15998   79636     na        1670   1518      0      0      0      0   \n",
       "15998  15999     110     na          36     32      0      0      0      0   \n",
       "15999  16000       8      0           6      4      2      2      0      0   \n",
       "\n",
       "      ag_002  ...  ee_002  ee_003  ee_004  ee_005   ee_006  ee_007  ee_008  \\\n",
       "0          0  ...    1098     138     412     654       78      88       0   \n",
       "1          0  ...    1068     276    1620     116       86     462       0   \n",
       "2          0  ...  495076  380368  440134  269556  1315022  153680     516   \n",
       "3          0  ...  540820  243270  483302  485332   431376  210074  281662   \n",
       "4          0  ...    7646    4144   18466   49782     3176     482      76   \n",
       "...      ...  ...     ...     ...     ...     ...      ...     ...     ...   \n",
       "15995      0  ...  632658  273242  510354  373918   349840  317840  960024   \n",
       "15996      0  ...     266      44      46      14        2       0       0   \n",
       "15997      0  ...  806832  449962  778826  581558   375498  222866  358934   \n",
       "15998      0  ...     588     210     180     544     1004    1338      74   \n",
       "15999      0  ...      46      10      48      14       42      46       0   \n",
       "\n",
       "      ee_009 ef_000 eg_000  \n",
       "0          0      0      0  \n",
       "1          0      0      0  \n",
       "2          0      0      0  \n",
       "3       3232      0      0  \n",
       "4          0      0      0  \n",
       "...      ...    ...    ...  \n",
       "15995  25566      0      0  \n",
       "15996      0      0      0  \n",
       "15997  19548      0      0  \n",
       "15998      0      0      0  \n",
       "15999      0      0      0  \n",
       "\n",
       "[16000 rows x 171 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "#use test set to predict\n",
    "log_df = test_df[numerical_columns].replace('na', 0).astype(float).apply(np.log)\n",
    "log_df = log_df.replace(-np.inf, 0)\n",
    "log_df = log_df.replace(np.inf, 0)\n",
    "log_df = log_df.replace(np.nan, 0)\n",
    "\n",
    "exp_df = test_df[numerical_columns].replace('na', 0).astype(float).apply(np.exp)\n",
    "exp_df = exp_df.replace(-np.inf, 0)\n",
    "exp_df = exp_df.replace(np.inf, 0)\n",
    "exp_df = exp_df.replace(np.nan, 0)\n",
    "\n",
    "#create a new pandas df with the log and exp columns and name them accordingly\n",
    "log_df.columns = log_df.columns.map(lambda x: 'log_' + x)\n",
    "exp_df.columns = exp_df.columns.map(lambda x: 'exp_' + x)\n",
    "\n",
    "#concatenate the log and exp columns with the original df\n",
    "test_df = pd.concat([test_df, log_df, exp_df], axis=1)\n",
    "\n",
    "X = test_df[other_columns].replace('na', 0).astype(float).values\n",
    "\n",
    "\n",
    "X_transformed = StandardScaler().fit_transform(X)\n",
    "target_prediction_ = aug_model.predict(X_transformed)\n",
    "\n",
    "output = pd.DataFrame({'id': np.arange(1,len(target_prediction_.reshape(-1))+1),\"class\": labelize(target_prediction_.reshape(-1))})\n",
    "output.to_csv('categorical+relevant numerical values standardized neural network with custom obj func finetuned 20 epochs', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     19470\n",
      "           1       0.73      0.85      0.79       330\n",
      "\n",
      "    accuracy                           0.99     19800\n",
      "   macro avg       0.86      0.92      0.89     19800\n",
      "weighted avg       0.99      0.99      0.99     19800\n",
      "\n",
      "[[19367   103]\n",
      " [   50   280]]\n"
     ]
    }
   ],
   "source": [
    "#lets do xgboost with the augmented data\n",
    "import xgboost as xgb\n",
    "\n",
    "clf = xgb.XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=9,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.5,\n",
    "    colsample_bytree=0.5,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=1000,\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "target_prediction = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, target_prediction))\n",
    "\n",
    "print(confusion_matrix(y_test, target_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define custom scoring for CV\n",
    "from sklearn.metrics import make_scorer\n",
    "def custom_cost_score(y_true, target_prediction):\n",
    "    # Define your costs\n",
    "    Cost_1 = 10  # Cost for false positives\n",
    "    Cost_2 = 500  # Cost for false negatives\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, target_prediction).ravel()\n",
    "\n",
    "    # Calculate the total cost\n",
    "    total_cost = (Cost_1 * fp + Cost_2 * fn)\n",
    "    return total_cost\n",
    "\n",
    "# Create a scorer object\n",
    "custom_scorer = make_scorer(custom_cost_score, greater_is_better=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, device=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     feature_types=None, gamma=None,\n",
       "                                     grow_policy=None, importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None,...\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     multi_strategy=None, n_estimators=-1,\n",
       "                                     n_jobs=None, num_parallel_tree=None,\n",
       "                                     random_state=42, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;colsample_bytree&#x27;: [0.5, 1.0],\n",
       "                         &#x27;learning_rate&#x27;: [0.01, 0.1], &#x27;max_depth&#x27;: [3],\n",
       "                         &#x27;scale_pos_weight&#x27;: [5, 50, 100, 500, 100],\n",
       "                         &#x27;subsample&#x27;: [0.5, 1.0]},\n",
       "             scoring=make_scorer(custom_cost_score, greater_is_better=False),\n",
       "             verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, device=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     feature_types=None, gamma=None,\n",
       "                                     grow_policy=None, importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None,...\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     multi_strategy=None, n_estimators=-1,\n",
       "                                     n_jobs=None, num_parallel_tree=None,\n",
       "                                     random_state=42, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;colsample_bytree&#x27;: [0.5, 1.0],\n",
       "                         &#x27;learning_rate&#x27;: [0.01, 0.1], &#x27;max_depth&#x27;: [3],\n",
       "                         &#x27;scale_pos_weight&#x27;: [5, 50, 100, 500, 100],\n",
       "                         &#x27;subsample&#x27;: [0.5, 1.0]},\n",
       "             scoring=make_scorer(custom_cost_score, greater_is_better=False),\n",
       "             verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=-1, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=42, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=-1, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, device=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     feature_types=None, gamma=None,\n",
       "                                     grow_policy=None, importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None,...\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     multi_strategy=None, n_estimators=-1,\n",
       "                                     n_jobs=None, num_parallel_tree=None,\n",
       "                                     random_state=42, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.5, 1.0],\n",
       "                         'learning_rate': [0.01, 0.1], 'max_depth': [3],\n",
       "                         'scale_pos_weight': [5, 50, 100, 500, 100],\n",
       "                         'subsample': [0.5, 1.0]},\n",
       "             scoring=make_scorer(custom_cost_score, greater_is_better=False),\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "\n",
    "X = set_training[other_columns].replace('na', 0).astype(float).values\n",
    "X_transformed = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'max_depth': [3, ],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.5, 1.0],\n",
    "    'colsample_bytree': [0.5, 1.0],\n",
    "    'scale_pos_weight': [5, 50 ,100, 500, 100]\n",
    "}\n",
    "\n",
    "# Initialize the XGBClassifier\n",
    "clf = xgb.XGBClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=-1,\n",
    "  )\n",
    "\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=clf,\n",
    "    param_grid=param_grid,\n",
    "    scoring=custom_scorer,\n",
    "    cv=3,  # Number of cross-validation folds (3-fold cross-validation)\n",
    "    verbose=3,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "grid_search.fit(X_transformed, target_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18000.0"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, target_value, test_size=0.33, random_state=42, stratify=target_value)\n",
    "\n",
    "clf = xgb.XGBClassifier(\n",
    "    n_estimators=-1,\n",
    "    random_state=42,\n",
    "\n",
    "    **grid_search.best_params_\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=-1, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=42, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=-1, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=-1, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=42, ...)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99     19470\n",
      "           1       0.00      0.00      0.00       330\n",
      "\n",
      "    accuracy                           0.98     19800\n",
      "   macro avg       0.49      0.50      0.50     19800\n",
      "weighted avg       0.97      0.98      0.98     19800\n",
      "\n",
      "[[19470     0]\n",
      " [  330     0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mt_er\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mt_er\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\mt_er\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "target_prediction = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, target_prediction))\n",
    "\n",
    "print(confusion_matrix(y_test, target_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97     19470\n",
      "           1       0.21      0.97      0.34       330\n",
      "\n",
      "    accuracy                           0.94     19800\n",
      "   macro avg       0.60      0.95      0.65     19800\n",
      "weighted avg       0.99      0.94      0.96     19800\n",
      "\n",
      "[[18249  1221]\n",
      " [   10   320]]\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000, max_depth=6, random_state=42, class_weight={0: 1, 1: 500})\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "target_prediction = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, target_prediction))\n",
    "\n",
    "print(confusion_matrix(y_test, target_prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97     19470\n",
      "           1       0.23      0.96      0.37       330\n",
      "\n",
      "    accuracy                           0.95     19800\n",
      "   macro avg       0.61      0.95      0.67     19800\n",
      "weighted avg       0.99      0.95      0.96     19800\n",
      "\n",
      "[[18403  1067]\n",
      " [   13   317]]\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000, max_depth=6, random_state=42, class_weight={0: 1, 1: 250})\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "target_prediction = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, target_prediction))\n",
    "\n",
    "print(confusion_matrix(y_test, target_prediction))\n",
    "\n",
    "\n",
    "#use test set to predict\n",
    "X = test_df[other_columns].replace('na', 0).astype(float).values\n",
    "X_transformed = StandardScaler().fit_transform(X)\n",
    "target_prediction_ = clf.predict(X_transformed)\n",
    "\n",
    "output = pd.DataFrame({'id': np.arange(1,len(target_prediction_.reshape(-1))+1),\"class\": labelize(target_prediction_.reshape(-1))})\n",
    "output.to_csv('randomforestLDA2.2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
